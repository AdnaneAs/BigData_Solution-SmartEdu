{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import Counter\n",
    "from deepface import DeepFace\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import base64\n",
    "import ffmpeg\n",
    "import time\n",
    "\n",
    "\n",
    "# Step 1: Split video to frames with OpenCV\n",
    "def split_video_to_frames(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    framerate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frames = []\n",
    "    selected_frames = []\n",
    "\n",
    "    # Calcul du nombre total de frames dans la vidéo\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calcule l'intervalle entre chaque image à extraire\n",
    "    intervalle = total_frames // 6\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    for i in range(6):\n",
    "        # Définit le numéro de frame à extraire\n",
    "        frame_num = intervalle * i\n",
    "        selected_frames.append(frames[frame_num])\n",
    "\n",
    "    cap.release()\n",
    "    return frames,selected_frames, framerate\n",
    "\n",
    "\n",
    "# in this fucntion all the work is done here so that in spark we launch only dowork()\n",
    "\n",
    "def dowork(x):\n",
    "    # Step 2: Take x frames as a list and select a frame as the original\n",
    "    # video_path = \"classrom1.mp4\"\n",
    "    video_path = x\n",
    "    pp = video_paths[0].split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    all_frames,selected_frames, original_frame_rate = split_video_to_frames(video_path)\n",
    "    # selected_frames = all_frames[:10]  # Replace x with the desired number\n",
    "    original_frame = selected_frames[0]\n",
    "\n",
    "    # Step 3: Original frame with histogram equalization\n",
    "\n",
    "    # Convert the image to YUV color space\n",
    "    yuv_image = cv2.cvtColor(original_frame, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Apply histogram equalization to the Y channel (luminance)\n",
    "    yuv_image[:, :, 0] = cv2.equalizeHist(yuv_image[:, :, 0])\n",
    "\n",
    "    # Convert the image back to BGR color space\n",
    "    original_frame_equalized = cv2.cvtColor(yuv_image, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "    # Step 4: Original frame with Bilateral filter (adjust the parameters if needed)\n",
    "    original_frame_denoised = cv2.bilateralFilter(original_frame, 9, 75, 75)\n",
    "\n",
    "    # Step 5: Original frame with Canny filter\n",
    "    original_frame_canny = cv2.Canny(cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY), 50, 150)\n",
    "\n",
    "    # Step 6: Use DeepFace for face and age detection\n",
    "    # Step 7: Generate a new video with face and age detection\n",
    "    output_video_path = f\"/SmartEdu_src/result/video___{pp}.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, original_frame_rate,\n",
    "                          (original_frame.shape[1], original_frame.shape[0]))\n",
    "    # Load pre-trained face detection model\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Initialize a counter for emotion frequencies\n",
    "    emotion_counter = Counter()\n",
    "\n",
    "    # Assuming 'all_frames' is a list of frames obtained from a video\n",
    "    for frame in all_frames:\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Process each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face region\n",
    "            face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Analyze the face for age and emotion\n",
    "            result = DeepFace.analyze(face_roi, actions=['age', 'emotion'], enforce_detection=False)\n",
    "\n",
    "            # Access age and emotion information for each face\n",
    "            age = result[0]['age']  # Access the first face in the list\n",
    "            emotion = result[0]['dominant_emotion']  # Access the first face in the list\n",
    "\n",
    "            # Update the emotion counter\n",
    "            emotion_counter[emotion] += 1\n",
    "\n",
    "            # Draw bounding box around the face with age and emotion information\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Age: {age}, Emotion: {emotion}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2)\n",
    "\n",
    "        # Save the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame_equalized___{pp}.jpg\", original_frame_equalized)\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame_denoised___{pp}.jpg\", original_frame_denoised)\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame_canny___{pp}.jpg\", original_frame_canny)\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame___{pp}.jpg\", original_frame)\n",
    "\n",
    "\n",
    "\n",
    "    # Print the emotion frequencies\n",
    "    print(\"Emotion Frequencies:\")\n",
    "    for emotion, count in emotion_counter.items():\n",
    "        print(f\"{emotion}: {count}\")\n",
    "\n",
    "\n",
    "\n",
    "    return \"Emotion Frequencies:\", emotion_counter.items()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
